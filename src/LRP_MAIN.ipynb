{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Activation\n",
    "from keras import backend as K\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_MODEL_JSON = '../saved_model/model_lstm.json'\n",
    "LSTM_MODEL_WEIGHTS = '../saved_model/model_lstm.h5'\n",
    "HISTORY_FILE = '../saved_model/history_lstm.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lstm_model(model):\n",
    "    # load json and create model\n",
    "    model_json = model.to_json()\n",
    "    with open(LSTM_MODEL_JSON, 'w') as jsonfile:\n",
    "        jsonfile.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(LSTM_MODEL_WEIGHTS)\n",
    "\n",
    "def load_lstm_model(model):\n",
    "    # load weights into new model\n",
    "    loaded_model = model_from_json(LSTM_MODEL_JSON)\n",
    "    loaded_model.load_weights(LSTM_MODEL_WEIGHTS)\n",
    "    # evaluate loaded model on test data\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/final_data_less.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()\n",
    "t.fit_on_texts(df['clean_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7051\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index)+1\n",
    "encoded_docs = t.texts_to_sequences(df['clean_sentiment'])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.get_dummies(df['sentiment'].values)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(x) for x in encoded_docs])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "Training Shape:  (800, 580) == Train Lables:  (800, 2)\n",
      "Test Shape:  (200, 580) == Test Lables:  (200, 2)\n"
     ]
    }
   ],
   "source": [
    "split_fraction = 0.8\n",
    "split_idx = int(len(padded_docs)*split_fraction)\n",
    "print(split_idx)\n",
    "\n",
    "X_train, X_test = padded_docs[:split_idx], padded_docs[split_idx:]\n",
    "y_train, y_test = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "print(\"Training Shape: \", X_train.shape, \"== Train Lables: \", y_train.shape)\n",
    "print(\"Test Shape: \", X_test.shape, \"== Test Lables: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 580, 128)          902528    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 74240)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               7424100   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 8,326,830\n",
      "Trainable params: 8,326,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,128,input_length=max_length))\n",
    "model.add(Flatten());\n",
    "model.add(Dense(100, activation='relu')); # input_shape=(max_words,)\n",
    "model.add(Dropout(0.5));\n",
    "model.add(Dense(2, activation='softmax'));\n",
    "# model.add(Dense(4, activation='linear'));\n",
    "model.add(Activation('softmax'));\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']);\n",
    "print(model.summary());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0136 - acc: 0.9988 - val_loss: 0.5297 - val_acc: 0.7300\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.5343 - val_acc: 0.7400\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.5421 - val_acc: 0.7550\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.5423 - val_acc: 0.7350\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.5481 - val_acc: 0.7300\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1,\n",
    "          shuffle=True);\n",
    "\n",
    "# predict on test data\n",
    "y_pred = model.predict(np.array(X_test));\n",
    "y_test = np.array(y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.50%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_lstm_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_session = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 580, 128)          902528    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 74240)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               7424100   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 8,326,830\n",
      "Trainable params: 8,326,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 13s 16ms/step - loss: 0.8066 - acc: 0.4825 - val_loss: 0.8605 - val_acc: 0.4500\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 12s 16ms/step - loss: 0.6444 - acc: 0.6738 - val_loss: 0.5749 - val_acc: 0.7600\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 0.3677 - acc: 0.9712 - val_loss: 0.5347 - val_acc: 0.7850\n",
      "Epoch 4/5\n",
      "610/800 [=====================>........] - ETA: 2s - loss: 0.3209 - acc: 0.9951"
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=current_session) as de:  # <-- init DeepExplain context\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,128,input_length=max_length))\n",
    "    model.add(Flatten());\n",
    "    model.add(Dense(100, activation='relu')); # input_shape=(max_words,)\n",
    "    model.add(Dropout(0.5));\n",
    "    model.add(Dense(2, activation='softmax'));\n",
    "    # model.add(Dense(4, activation='linear'));\n",
    "    model.add(Activation('softmax'));\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy']);\n",
    "    print(model.summary());\n",
    "    model.fit(X_train, y_train,\n",
    "          batch_size=10,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1,\n",
    "          shuffle=True);\n",
    "\n",
    "    # predict on test data\n",
    "    y_pred = model.predict(np.array(X_test));\n",
    "    y_test = np.array(y_test);\n",
    "    \n",
    "    # Evaluate the embedding tensor on the model input (in other words, perform the lookup)\n",
    "    embedding_tensor = model.layers[0].output\n",
    "    input_tensor = model.inputs[0]\n",
    "    embedding_out = current_session.run(embedding_tensor, {input_tensor: X_test});\n",
    "\n",
    "    xs = X_test;\n",
    "    ys = y_test;\n",
    "    # Run DeepExplain with the embedding as input\n",
    "    attributions = de.explain('elrp', model.layers[-2].output * ys, model.layers[1].input, embedding_out);\n",
    "    print(\"attributions shape --- {}\".format(attributions.shape));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
