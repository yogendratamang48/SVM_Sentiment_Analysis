{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Activation\n",
    "from keras import backend as K\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "LSTM_MODEL_JSON = '../saved_model/model_lstm.json'\n",
    "LSTM_MODEL_WEIGHTS = '../saved_model/model_lstm.h5'\n",
    "HISTORY_FILE = '../saved_model/history_lstm.json'\n",
    "\n",
    "def save_lstm_model(model):\n",
    "    # load json and create model\n",
    "    model_json = model.to_json()\n",
    "    with open(LSTM_MODEL_JSON, 'w') as jsonfile:\n",
    "        jsonfile.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(LSTM_MODEL_WEIGHTS)\n",
    "\n",
    "def load_lstm_model(model):\n",
    "    # load weights into new model\n",
    "    loaded_model = model_from_json(LSTM_MODEL_JSON)\n",
    "    loaded_model.load_weights(LSTM_MODEL_WEIGHTS)\n",
    "    # evaluate loaded model on test data\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/final_data_less.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()\n",
    "t.fit_on_texts(df['clean_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7051\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index)+1\n",
    "encoded_docs = t.texts_to_sequences(df['clean_sentiment'])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.get_dummies(df['sentiment'].values)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(x) for x in encoded_docs])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "Training Shape:  (800, 580) == Train Lables:  (800, 2)\n",
      "Test Shape:  (200, 580) == Test Lables:  (200, 2)\n"
     ]
    }
   ],
   "source": [
    "split_fraction = 0.8\n",
    "split_idx = int(len(padded_docs)*split_fraction)\n",
    "print(split_idx)\n",
    "\n",
    "X_train, X_test = padded_docs[:split_idx], padded_docs[split_idx:]\n",
    "y_train, y_test = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "print(\"Training Shape: \", X_train.shape, \"== Train Lables: \", y_train.shape)\n",
    "print(\"Test Shape: \", X_test.shape, \"== Test Lables: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_session = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 580, 128)          902528    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 74240)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               7424100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 8,326,830\n",
      "Trainable params: 8,326,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 14s 17ms/step - loss: 0.8623 - acc: 0.5050 - val_loss: 0.6920 - val_acc: 0.5500\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 14s 17ms/step - loss: 0.6868 - acc: 0.5413 - val_loss: 0.6795 - val_acc: 0.5550\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 0.4376 - acc: 0.8125 - val_loss: 0.4826 - val_acc: 0.7550\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 13s 16ms/step - loss: 0.0389 - acc: 0.9925 - val_loss: 0.4949 - val_acc: 0.7600\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.5497 - val_acc: 0.7750\n",
      "DeepExplain: running \"elrp\" explanation method (4)\n",
      "Model with multiple inputs:  False\n",
      "attributions shape --- (200, 580, 128)\n"
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=current_session) as de:  # <-- init DeepExplain context\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,128,input_length=max_length))\n",
    "    model.add(Flatten());\n",
    "    model.add(Dense(100, activation='relu')); # input_shape=(max_words,)\n",
    "    model.add(Dropout(0.5));\n",
    "    model.add(Dense(2, activation='linear'));\n",
    "#     model.add(Dense(4, activation='linear'));\n",
    "    model.add(Activation('softmax'));\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy']);\n",
    "    print(model.summary());\n",
    "    model.fit(X_train, y_train,\n",
    "          batch_size=10,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1,\n",
    "          shuffle=True);\n",
    "\n",
    "    # predict on test data\n",
    "    y_pred = model.predict(np.array(X_test));\n",
    "    y_test = np.array(y_test);\n",
    "    \n",
    "    # Evaluate the embedding tensor on the model input (in other words, perform the lookup)\n",
    "    embedding_tensor = model.layers[0].output\n",
    "    input_tensor = model.inputs[0]\n",
    "    embedding_out = current_session.run(embedding_tensor, {input_tensor: X_test});\n",
    "\n",
    "    xs = X_test;\n",
    "    ys = y_test;\n",
    "    # Run DeepExplain with the embedding as input\n",
    "    attributions = de.explain('elrp', model.layers[-2].output * ys, model.layers[1].input, embedding_out);\n",
    "    print(\"attributions shape --- {}\".format(attributions.shape));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_lstm_model(model)\n",
    "# model = load_lstm_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.50%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions.shape\n",
    "np.save('att.npy', attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.load('att.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.05364845e-03, -1.53772701e-02, -5.45059983e-03, ...,\n",
       "         -1.54183607e-03, -1.01193618e-02,  3.61893559e-03],\n",
       "        [ 9.30683874e-03,  3.53212527e-04,  3.47623206e-03, ...,\n",
       "          1.15158819e-02, -1.81808844e-02, -5.22844086e-04],\n",
       "        [ 1.13056116e-02,  1.76490226e-04,  3.33558116e-03, ...,\n",
       "         -2.28086035e-04, -9.75910516e-04, -1.01954732e-02],\n",
       "        ...,\n",
       "        [ 2.30018577e-06,  1.35805021e-05, -1.77738693e-05, ...,\n",
       "          4.18761592e-05, -4.41564316e-06, -3.75883269e-06],\n",
       "        [-3.40495092e-07, -2.05347487e-06,  1.09185545e-04, ...,\n",
       "          4.80300077e-05,  1.02032027e-06, -5.78749678e-05],\n",
       "        [-7.90705781e-06, -1.04315286e-05, -1.97287227e-05, ...,\n",
       "          5.78051040e-05,  6.43962994e-06,  9.71091595e-06]],\n",
       "\n",
       "       [[-4.01770510e-03,  4.41142730e-03,  8.53756908e-03, ...,\n",
       "         -3.47694801e-03, -1.84383837e-03,  2.12098262e-03],\n",
       "        [ 1.79074123e-03,  4.04153904e-03,  2.26298980e-02, ...,\n",
       "         -2.41252617e-03, -3.47280712e-03,  2.11962336e-03],\n",
       "        [ 7.12086819e-03, -8.65183712e-04,  1.79481064e-03, ...,\n",
       "         -2.52902682e-04, -3.35962308e-04,  1.48215261e-03],\n",
       "        ...,\n",
       "        [-1.03432067e-05,  1.63767327e-05,  1.51538814e-04, ...,\n",
       "          7.22926488e-05,  1.02755530e-05, -4.29823667e-05],\n",
       "        [ 1.17428635e-05,  2.60779575e-06,  7.95421947e-05, ...,\n",
       "          6.35212928e-05,  7.39839970e-06,  9.76576212e-06],\n",
       "        [ 1.34451320e-05,  9.35718253e-06,  1.55878559e-04, ...,\n",
       "          1.02109800e-04,  8.50713514e-06,  1.48707511e-06]],\n",
       "\n",
       "       [[ 4.51438408e-03, -1.30570745e-02, -1.34011195e-03, ...,\n",
       "          8.07044003e-03,  1.37107901e-03, -3.36523540e-03],\n",
       "        [ 1.43098561e-02, -2.72951438e-04,  8.15406907e-03, ...,\n",
       "         -1.85867317e-03, -1.14548989e-02, -1.20771362e-03],\n",
       "        [ 5.06363297e-03, -1.20516765e-04, -4.39548079e-04, ...,\n",
       "         -1.47063262e-03,  4.40964475e-04,  7.67613435e-03],\n",
       "        ...,\n",
       "        [ 2.66353140e-07,  1.35313085e-05, -1.40242155e-05, ...,\n",
       "          4.09502900e-05, -4.40353961e-06, -1.15175590e-06],\n",
       "        [ 4.52595913e-07, -2.25685312e-06,  1.09032691e-04, ...,\n",
       "          4.68462022e-05,  1.05147592e-06, -5.60682602e-05],\n",
       "        [-7.35499407e-06, -1.10942010e-05, -2.27574019e-05, ...,\n",
       "          5.77666688e-05,  6.29923079e-06,  8.52813810e-06]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-8.40867870e-03,  7.65501661e-03, -1.24743190e-02, ...,\n",
       "          2.15646322e-03, -1.97205343e-03, -1.99065602e-04],\n",
       "        [ 4.17678989e-03,  2.58232397e-03,  1.10545894e-02, ...,\n",
       "         -1.30693044e-03, -6.30455138e-03,  3.24847084e-03],\n",
       "        [ 2.36612256e-03, -2.16235500e-03,  5.51280333e-03, ...,\n",
       "          6.68288121e-05,  1.34415961e-06, -1.35046639e-03],\n",
       "        ...,\n",
       "        [-1.37039588e-05,  1.35209420e-05,  1.52116729e-04, ...,\n",
       "          5.58188513e-05,  1.07289743e-05, -3.83950392e-05],\n",
       "        [ 1.03599095e-05, -1.01214887e-07,  7.56684894e-05, ...,\n",
       "          4.59520525e-05,  7.19190211e-06,  1.06719299e-05],\n",
       "        [ 1.30260360e-05,  6.24572203e-06,  1.49343294e-04, ...,\n",
       "          9.27335495e-05,  8.14902523e-06,  4.94866072e-06]],\n",
       "\n",
       "       [[ 9.06750094e-03, -1.53757324e-02, -5.45190834e-03, ...,\n",
       "         -1.54713704e-03, -1.01290951e-02,  3.62676312e-03],\n",
       "        [-6.62475172e-03, -5.00006892e-04,  1.35532450e-02, ...,\n",
       "         -6.47980394e-03, -4.75529628e-03, -3.47748725e-03],\n",
       "        [ 3.14726750e-03,  1.30471090e-04,  4.49040148e-04, ...,\n",
       "          2.81998917e-04,  9.45345790e-04, -6.39282120e-03],\n",
       "        ...,\n",
       "        [ 2.29422562e-06,  1.33879557e-05, -1.78945829e-05, ...,\n",
       "          4.05531136e-05, -4.38117058e-06, -3.48571348e-06],\n",
       "        [-3.68686017e-07, -2.27424016e-06,  1.09242981e-04, ...,\n",
       "          4.67258069e-05,  1.06105483e-06, -5.77566134e-05],\n",
       "        [-8.03037710e-06, -1.06529651e-05, -1.98853231e-05, ...,\n",
       "          5.68196047e-05,  6.45475257e-06,  9.85306633e-06]],\n",
       "\n",
       "       [[ 4.57448000e-03, -2.63679144e-03,  1.89605739e-03, ...,\n",
       "          2.48943246e-03, -3.58308735e-03,  2.87705264e-03],\n",
       "        [ 6.01214310e-03, -2.35533901e-03, -2.05016769e-02, ...,\n",
       "          6.56144181e-03, -5.26515022e-03,  3.64922220e-04],\n",
       "        [-2.34810729e-03,  6.14610093e-04, -9.32801864e-04, ...,\n",
       "          2.66597362e-06,  5.83527784e-04,  9.24446154e-04],\n",
       "        ...,\n",
       "        [-1.30117651e-05,  1.65791080e-05,  1.56786802e-04, ...,\n",
       "          7.29024687e-05,  1.02481554e-05, -3.98870834e-05],\n",
       "        [ 1.28421661e-05,  2.63935021e-06,  7.93396684e-05, ...,\n",
       "          6.37640915e-05,  7.39001280e-06,  1.20168497e-05],\n",
       "        [ 1.43431052e-05,  8.77890761e-06,  1.52170251e-04, ...,\n",
       "          1.03432110e-04,  8.30582121e-06, -2.61380507e-07]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_TEST = '../data/test_less.csv'\n",
    "def get_test_sentence(sent_idx):\n",
    "    \"\"\"\n",
    "    Returns a test set sentence and its label, sent_idx must be an integer in [1, 2210]\"\"\"\n",
    "    _df = pd.read_csv(SEQUENCE_TEST)\n",
    "    print(\"Shape: \",_df.shape)\n",
    "#     sentence = _df['reviewText'][sent_idx]\n",
    "#     print(\"Raw Sentiment\\n\")\n",
    "#     print(sentence)\n",
    "    sentiment = _df['sentiment'][sent_idx]\n",
    "    sentence_ = _df['clean_sentiment'][sent_idx]\n",
    "    sent_array = y_test[sent_idx]\n",
    "    print(\"Clean Texts:\\n\")\n",
    "    print(sentence_)\n",
    "    print(\"Sentiment: \", sentiment)\n",
    "    clean_words = sentence_.split()\n",
    "    return clean_words, sentiment\n",
    "\n",
    "def find_score(sent_idx):\n",
    "    sent_words, sent_sentiment = get_test_sentence(sent_idx)\n",
    "    scores = []\n",
    "    for idx, word in enumerate(sent_words):\n",
    "        print(word, \": \", b[sent_idx][idx].sum())\n",
    "        scores.append(b[sent_idx][idx].sum())\n",
    "    scores = np.array(scores)\n",
    "    return sent_words, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (200, 3)\n",
      "Clean Texts:\n",
      "\n",
      "got 3m old son although seem fit body fine wasnt able sit without assistance thought 5th month probably wont fit body practical also long height seem like tip easily even straps didnt feel comfortable using without supervises would recommend\n",
      "Sentiment:  0\n"
     ]
    }
   ],
   "source": [
    "sent_words, sent_sentiment = get_test_sentence(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.heatmap import html_heatmap\n",
    "\n",
    "import codecs\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (200, 3)\n",
      "Clean Texts:\n",
      "\n",
      "got 3m old son although seem fit body fine wasnt able sit without assistance thought 5th month probably wont fit body practical also long height seem like tip easily even straps didnt feel comfortable using without supervises would recommend\n",
      "Sentiment:  0\n",
      "got :  -0.05375789\n",
      "3m :  0.037802503\n",
      "old :  -0.24430352\n",
      "son :  -0.29903167\n",
      "although :  0.080530465\n",
      "seem :  0.00020494871\n",
      "fit :  -0.05892004\n",
      "body :  -0.047177978\n",
      "fine :  0.15070361\n",
      "wasnt :  -0.021080611\n",
      "able :  -0.1420303\n",
      "sit :  0.07942906\n",
      "without :  -0.3722966\n",
      "assistance :  0.040475845\n",
      "thought :  0.26360533\n",
      "5th :  0.0012445571\n",
      "month :  0.043012396\n",
      "probably :  0.07722144\n",
      "wont :  0.05125931\n",
      "fit :  0.12899534\n",
      "body :  -0.072885126\n",
      "practical :  0.06041016\n",
      "also :  -0.087189354\n",
      "long :  -0.02296719\n",
      "height :  -0.1245781\n",
      "seem :  -0.14926124\n",
      "like :  -0.05637505\n",
      "tip :  -0.11973221\n",
      "easily :  -0.010592928\n",
      "even :  -0.040554233\n",
      "straps :  0.026007628\n",
      "didnt :  -0.0029238444\n",
      "feel :  -0.0011834865\n",
      "comfortable :  -0.0707767\n",
      "using :  0.03204188\n",
      "without :  -0.03337615\n",
      "supervises :  -0.006357707\n",
      "would :  0.094616964\n",
      "recommend :  0.060885005\n"
     ]
    }
   ],
   "source": [
    "words_, scores_ = find_score(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.549731125831604, 0.775]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
