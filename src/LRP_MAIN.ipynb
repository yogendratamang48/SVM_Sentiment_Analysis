{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Activation\n",
    "from keras import backend as K\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "LSTM_MODEL_JSON = '../saved_model/model_lstm.json'\n",
    "LSTM_MODEL_WEIGHTS = '../saved_model/model_lstm.h5'\n",
    "HISTORY_FILE = '../saved_model/history_lstm.json'\n",
    "\n",
    "def save_lstm_model(model):\n",
    "    # load json and create model\n",
    "    model_json = model.to_json()\n",
    "    with open(LSTM_MODEL_JSON, 'w') as jsonfile:\n",
    "        jsonfile.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(LSTM_MODEL_WEIGHTS)\n",
    "\n",
    "def load_lstm_model(model):\n",
    "    # load weights into new model\n",
    "    loaded_model = model_from_json(LSTM_MODEL_JSON)\n",
    "    loaded_model.load_weights(LSTM_MODEL_WEIGHTS)\n",
    "    # evaluate loaded model on test data\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/final_data_less.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()\n",
    "t.fit_on_texts(df['clean_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7051\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index)+1\n",
    "encoded_docs = t.texts_to_sequences(df['clean_sentiment'])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.get_dummies(df['sentiment'].values)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(x) for x in encoded_docs])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "Training Shape:  (800, 580) == Train Lables:  (800, 2)\n",
      "Test Shape:  (200, 580) == Test Lables:  (200, 2)\n"
     ]
    }
   ],
   "source": [
    "split_fraction = 0.8\n",
    "split_idx = int(len(padded_docs)*split_fraction)\n",
    "print(split_idx)\n",
    "\n",
    "X_train, X_test = padded_docs[:split_idx], padded_docs[split_idx:]\n",
    "y_train, y_test = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "print(\"Training Shape: \", X_train.shape, \"== Train Lables: \", y_train.shape)\n",
    "print(\"Test Shape: \", X_test.shape, \"== Test Lables: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_session = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 580, 128)          902528    \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 74240)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               7424100   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 8,326,830\n",
      "Trainable params: 8,326,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "800/800 [==============================] - 13s 16ms/step - loss: 0.8533 - acc: 0.4950 - val_loss: 0.6868 - val_acc: 0.5500\n",
      "Epoch 2/5\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.6182 - acc: 0.6800 - val_loss: 0.6093 - val_acc: 0.6700\n",
      "Epoch 3/5\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.1266 - acc: 0.9662 - val_loss: 0.5398 - val_acc: 0.7100\n",
      "Epoch 4/5\n",
      "800/800 [==============================] - 15s 19ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.5670 - val_acc: 0.7500\n",
      "Epoch 5/5\n",
      "800/800 [==============================] - 13s 16ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.5720 - val_acc: 0.7550\n",
      "DeepExplain: running \"elrp\" explanation method (4)\n",
      "Model with multiple inputs:  False\n",
      "attributions shape --- (200, 580, 128)\n"
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=current_session) as de:  # <-- init DeepExplain context\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,128,input_length=max_length))\n",
    "    model.add(Flatten());\n",
    "    model.add(Dense(100, activation='relu')); # input_shape=(max_words,)\n",
    "    model.add(Dropout(0.5));\n",
    "    model.add(Dense(2, activation='linear'));\n",
    "#     model.add(Dense(4, activation='linear'));\n",
    "    model.add(Activation('softmax'));\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy']);\n",
    "    print(model.summary());\n",
    "    model.fit(X_train, y_train,\n",
    "          batch_size=10,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1,\n",
    "          shuffle=True);\n",
    "\n",
    "    # predict on test data\n",
    "    y_pred = model.predict(np.array(X_test));\n",
    "    y_test = np.array(y_test);\n",
    "    \n",
    "    # Evaluate the embedding tensor on the model input (in other words, perform the lookup)\n",
    "    embedding_tensor = model.layers[0].output\n",
    "    input_tensor = model.inputs[0]\n",
    "    embedding_out = current_session.run(embedding_tensor, {input_tensor: X_test});\n",
    "\n",
    "    xs = X_test;\n",
    "    ys = y_test;\n",
    "    # Run DeepExplain with the embedding as input\n",
    "    attributions = de.explain('elrp', model.layers[-2].output * ys, model.layers[1].input, embedding_out);\n",
    "    print(\"attributions shape --- {}\".format(attributions.shape));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_lstm_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.50%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 580, 128)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
